{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube URL Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface\n",
    "### Data descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_size = 100000000 # specifies the sample size\n",
    "sample_size = 100000 # specifies the sample size\n",
    "limit = 15 # specifies the number of elements in lists and graphs\n",
    "\n",
    "url_columns = [\"id\",\"hash\",\"channelId\",\"publishedAt\",\"url\",\"resolvedUrl\",\"domain\",\"resolved\",\"dead\",\"status_code\"]\n",
    "video_columns = [\"id\",\"hash\",\"channelId\",\"channelTitle\",\"publishedAt\",\"title\",\"description\",\"tags\",\"categoryId\",\"defaultLanguage\",\"defaultAudioLanguage\",\"duration\",\"dimension\",\"definition\",\"caption\",\"licensedContent\",\"allowedIn\",\"blockedIn\",\"viewCount\",\"likeCount\",\"dislikeCount\",\"favoriteCount\",\"commentCount\",\"requesttime\"]\n",
    "channel_columns = [\"id\",\"title\",\"description\",\"subscriberCount\",\"videoCount\",\"commentCount\",\"viewCount\",\"featuring\",\"subscribing\",\"country\",\"publishedAt\",\"daysactive\",\"keywords\",\"topics\",\"wikitopics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(14,8.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and clean URLs csv\n",
    "pd_url_data = pd.read_csv('sample_' + str(sample_size) + '.csv')\n",
    "pd_url_data.columns = url_columns\n",
    "\n",
    "# add year column, based on publishedAt\n",
    "pd_url_data['year'] = pd.to_datetime(pd_url_data[\"publishedAt\"]).dt.year\n",
    "pd_url_data['date'] = pd.to_datetime(pd_url_data[\"publishedAt\"]).dt.date\n",
    "\n",
    "# skip everything before december 2019\n",
    "pd_url_data = pd_url_data[pd_url_data['date'] < datetime.date(year=2019,month=12,day=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean channel data\n",
    "pd_channel_data = pd.read_csv('channel_data/channels_from_jsoncache_100k_update.csv')\n",
    "pd_channel_data.columns = channel_columns\n",
    "\n",
    "# join with url data on channelId\n",
    "pd_url_data = pd_url_data.join(pd_channel_data.set_index('id'), on='channelId', lsuffix='_urls', rsuffix='_channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all www. or www2.\n",
    "pd_url_data.replace({'domain': r'^www2?\\.'}, {'domain': ''}, regex=True, inplace=True) \n",
    "\n",
    "# remove trailing slash\n",
    "pd_url_data.replace({'resolvedUrl': r'/+$'}, {'resolvedUrl':''}, regex=True, inplace=True)\n",
    "\n",
    "# replace all open.spotify.com by spotify.com\n",
    "pd_url_data.replace({'domain': r'^open\\.spotify'}, {'domain': 'spotify'}, regex=True, inplace=True) \n",
    "\n",
    "# @todo: check domains and see whether more needs to be replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min date: 2005-12-19, max date: 2019-11-30\n",
      "\n",
      "Basic stats on int/float columns\n",
      "                 id      resolved          dead   status_code          year  \\\n",
      "count  9.887400e+04  98874.000000  98874.000000  34653.000000  98874.000000   \n",
      "mean   2.885530e+08      0.842790      0.035672    224.943180   2016.883953   \n",
      "std    1.667773e+08      0.364001      0.185471     92.657309      2.005450   \n",
      "min    2.200000e+02      0.000000      0.000000      0.000000   2005.000000   \n",
      "25%    1.441704e+08      1.000000      0.000000    200.000000   2016.000000   \n",
      "50%    2.881955e+08      1.000000      0.000000    200.000000   2017.000000   \n",
      "75%    4.328208e+08      1.000000      0.000000    200.000000   2018.000000   \n",
      "max    5.777342e+08      1.000000      1.000000    999.000000   2019.000000   \n",
      "\n",
      "       subscriberCount     videoCount  commentCount     viewCount  \\\n",
      "count     9.887200e+04   98872.000000       98872.0  9.887200e+04   \n",
      "mean      1.485597e+06   22184.817734           0.0  7.570180e+08   \n",
      "std       3.828437e+06   44643.077366           0.0  2.819726e+09   \n",
      "min       1.000000e+05       0.000000           0.0  0.000000e+00   \n",
      "25%       2.060000e+05     953.750000           0.0  5.247182e+07   \n",
      "50%       4.600000e+05    3534.000000           0.0  1.517980e+08   \n",
      "75%       1.260000e+06   22178.000000           0.0  5.004309e+08   \n",
      "max       1.190000e+08  468380.000000           0.0  9.025180e+10   \n",
      "\n",
      "          featuring   subscribing    daysactive  \n",
      "count  98872.000000  98872.000000  98870.000000  \n",
      "mean       5.797840     33.590359   2652.122130  \n",
      "std        7.035186    117.595654   1153.784293  \n",
      "min        0.000000      0.000000     72.000000  \n",
      "25%        1.000000      0.000000   1791.000000  \n",
      "50%        4.000000      0.000000   2549.000000  \n",
      "75%        8.000000      7.000000   3343.000000  \n",
      "max      100.000000    996.000000   5328.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"min date: %s, max date: %s\\n\" % (min(pd_url_data.date),max(pd_url_data.date)))\n",
    "print(\"Basic stats on int/float columns\")\n",
    "print(pd_url_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check 'dead' URLs + how many have not yet been checked \n",
    "When less than 5%, just leave it as is\n",
    "\n",
    "@todo: too many unchecked URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLS: 98874, of which 3527 (3.57%) dead\n",
      "Total URLS: 98874, of which 12056 (12.19%) NOT yet checked\n"
     ]
    }
   ],
   "source": [
    "# count URLs marked as dead on expansion and sort in reverse\n",
    "pd_url_data_dead = len(pd_url_data.query(\"dead==1\"))\n",
    "# count total number of URLs\n",
    "total = len(pd_url_data)\n",
    "# calculate percentage\n",
    "percent_dead = round(pd_url_data_dead/total*100,2) \n",
    "# print percentage\n",
    "print(\"Total URLS: %s, of which %s (%s%%) dead\" % (total,pd_url_data_dead,percent_dead)) \n",
    "\n",
    "# unchecked if resolved = 0 and dead = 0\n",
    "pd_url_data_unchecked = len(pd_url_data.query('resolved==0 & dead==0'))\n",
    "percent_unchecked = round(pd_url_data_unchecked/total*100,2)\n",
    "print(\"Total URLS: %s, of which %s (%s%%) NOT yet checked\" % (total,pd_url_data_unchecked,percent_unchecked)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats (based on videos, not channels)\n",
    "### Top domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 domains\n",
      "domain\n",
      "youtube.com            31360\n",
      "facebook.com           11336\n",
      "twitter.com             9110\n",
      "instagram.com           5232\n",
      "google.com              2055\n",
      "twitch.tv                975\n",
      "vk.com                   960\n",
      "soundcloud.com           552\n",
      "play.google.com          532\n",
      "amazon.com               472\n",
      "itunes.apple.com         450\n",
      "accounts.google.com      434\n",
      "pinterest.com            421\n",
      "patreon.com              356\n",
      "amazon.de                286\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# add count column\n",
    "pd_url_data['count'] = 1 \n",
    "# count domains in URLs data set and sort in reverse\n",
    "pd_url_data_groupedDomain = pd_url_data.groupby([\"domain\"]).count()['count'].sort_values(ascending=False) \n",
    "# print header\n",
    "print(\"Top %s domains\" % (limit)) \n",
    "# print most used domains overall\n",
    "print(pd_url_data_groupedDomain.head(limit)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top deep links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 resolvedUrl\n",
      "resolvedUrl\n",
      "https://www.zee5.com                                     128\n",
      "https://www.youtube.com/error?src=404                     83\n",
      "https://www.youtube.com/user/euronewsnetwork/channels     80\n",
      "https://unacademy.com/unavailable                         77\n",
      "https://creativecommons.org/licenses/by/4.0/)             70\n",
      "http://incompetech.com                                    67\n",
      "https://www.facebook.com/euronews                         63\n",
      "https://twitter.com/ZEE5India                             58\n",
      "https://www.youtube.com/playlist?list...                  55\n",
      "https://datinglogic.net/forums/forum/qa                   54\n",
      "https://www.facebook.com/ZEE5                             54\n",
      "http://creativecommons.org/licenses/by/3.0                51\n",
      "http://www.epidemicsound.com                              50\n",
      "http://vod.sbs.co.kr                                      49\n",
      "https://www.instagram.com/zee5                            47\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# reset count column\n",
    "pd_url_data['count'] = 1 \n",
    "# count resolved deep links in URLs data set and sort in reverse\n",
    "pd_url_data_groupedUrl = pd_url_data.groupby([\"resolvedUrl\"]).count()['count'].sort_values(ascending=False) \n",
    "# print header\n",
    "print(\"Top %s resolvedUrl\" % (limit)) \n",
    "# print most used deep links\n",
    "print(pd_url_data_groupedUrl.head(limit)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top TLDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset count column\n",
    "pd_url_data['count'] = 1 \n",
    "# make TLD column based on domain\n",
    "pd_url_data['tld'] = pd_url_data['domain'].str.replace('^.*\\.', '', regex=True) \n",
    "# count TLDs used and sort in reverse\n",
    "pd_url_data_groupedTld = pd_url_data.groupby([\"tld\"]).count()['count'].sort_values(ascending=False)\n",
    "# print header\n",
    "print(\"Top %s TLDs\" % (limit))\n",
    "# print most used TLDs\n",
    "print(pd_url_data_groupedTld.head(limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top domains per TLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each tld, get limit domain\n",
    "print(\"Top domains per TLD\")\n",
    "for tld in pd_url_data_groupedTld.head(limit).keys():\n",
    "    top_domains_for_tld = pd_url_data[pd_url_data['tld']==tld].groupby(['tld','domain'])['hash'].nunique().sort_values(ascending=False)\n",
    "    print(top_domains_for_tld[:limit])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monetization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allowed merch and monetization on Github (by Bernhard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import merch and crowd lists\n",
    "from allowed_3rdparty import merch, crowd\n",
    "print(\"Number of merch sites: %s\" % (len(merch)))\n",
    "print(\"Number of crowd sites: %s\" % (len(crowd)))\n",
    "\n",
    "# add boolean column to pd_url_data based on presence in lists\n",
    "pd_url_data[\"crowd\"] = np.where(pd_url_data[\"domain\"].isin(crowd), True, False)\n",
    "pd_url_data[\"merch\"] = np.where(pd_url_data[\"domain\"].isin(merch), True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of videos that have at least one merchandise or one crowd-funding link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a column where at least one of monetized or crowd is true\n",
    "monetized = pd_url_data['merch'] | pd_url_data['crowd'] \n",
    "pd_url_data['monetized'] = monetized\n",
    "\n",
    "# get number of video ids where monetized = True\n",
    "monetized_videos = pd_url_data[pd_url_data['monetized']==True]['hash'].unique()\n",
    "monetized_video_number = len(monetized_videos)\n",
    "total_video_number = len(pd_url_data['hash'].unique())\n",
    "percent_video_monetized = round(monetized_video_number/total_video_number*100,2)\n",
    "# do the same for channels\n",
    "monetized_channels = pd_url_data[pd_url_data['monetized']==True]['channelId'].unique()\n",
    "monetized_channel_number = len(monetized_channels)\n",
    "total_channel_number = len(pd_url_data['channelId'].unique())\n",
    "percent_channel_monetized = round(monetized_channel_number/total_channel_number*100,2)\n",
    "\n",
    "print(\"%s videos are monetized out of %s total videos, that is %s%%\" % (monetized_video_number,total_video_number,percent_video_monetized))\n",
    "print(\"%s channels are monetized out of %s total channels, that is %s%%\" % (monetized_channel_number,total_channel_number,percent_channel_monetized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel and domain counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique videos for each channel\n",
    "videoCountForChannel = pd_url_data.groupby('channelId')['hash'].nunique()\n",
    "# add videoCountForChannel as column\n",
    "pd_url_data['videoCountForChannel'] = pd_url_data['channelId'].map(videoCountForChannel)\n",
    "\n",
    "# count unique videos for each domain\n",
    "videoCountForDomain = pd_url_data.groupby('domain')['hash'].nunique()\n",
    "# add videoCountForDomain as column\n",
    "pd_url_data['videoCountForDomain'] = pd_url_data['domain'].map(videoCountForDomain)\n",
    "\n",
    "# count unique channels for domain\n",
    "channelCountForDomain = pd_url_data.groupby('domain')['channelId'].nunique()\n",
    "# add channelCountForDomain as column\n",
    "pd_url_data['channelCountForDomain'] = pd_url_data['domain'].map(channelCountForDomain)\n",
    "\n",
    "# add log(channels/videos) for domains\n",
    "pd_url_data['channelDivVideoForDomain'] = pd_url_data['channelCountForDomain']/pd_url_data['videoCountForDomain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of YouTube videos per domain, per monetization type. As well as graph of domains per year, per monetization type\n",
    "Beware: below calculations assume that merch or crowd URLs are only mentioned once per video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_stats = {}\n",
    "for type in ['crowd','merch']:\n",
    "\n",
    "    ''' Describe '''\n",
    "    domain_stats[type] = pd_url_data[['domain','channelCountForDomain','videoCountForDomain','channelDivVideoForDomain']][pd_url_data[type]==True].drop_duplicates()\n",
    "\n",
    "    # print header\n",
    "    print(\"%s domain count\" % type)\n",
    "    # print most used TYPE domains\n",
    "    print(domain_stats[type].sort_values(by=['channelCountForDomain','channelDivVideoForDomain'],ascending=False).head(limit))\n",
    "    print(\"\\n\")\n",
    "    # print TYPE domain stats\n",
    "    print(\"%s domain stats\" % type)\n",
    "    print(domain_stats[type].describe())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    ''' Graph '''\n",
    "\n",
    "    # filter pd_url_data to only retain info for rows with TYPE domain\n",
    "    filtered = {}\n",
    "    filtered[type] = pd_url_data[pd_url_data[type]==True]\n",
    "\n",
    "    # limit data by top overall TYPE domain for graph\n",
    "    top_domains = {}\n",
    "    top_domains[type] = domain_stats[type].sort_values(by=\"channelCountForDomain\",ascending=False)['domain'][0:limit]\n",
    "    filtered_limited = {}\n",
    "    filtered_limited[type] = filtered[type][filtered[type]['domain'].isin(top_domains[type])==True]\n",
    "\n",
    "    # get number of unique domains for color palette in graph \n",
    "    num_domains = {}\n",
    "    num_domains[type] = len(filtered_limited[type]['domain'].unique())\n",
    "    color_palette = {}\n",
    "    color_palette[type] = sns.color_palette(\"Paired\",n_colors=num_domains[type])\n",
    "\n",
    "    # count videos by TYPE domain and year\n",
    "    filtered_limited_count = {}\n",
    "    filtered_limited_count[type] = filtered_limited[type].groupby([\"domain\",\"year\"])['hash'].nunique()\n",
    "    filtered_limited_count[type] = filtered_limited_count[type].to_frame()\n",
    "    filtered_limited_count[type].reset_index(inplace=True)\n",
    "    \n",
    "    # plot rise of TYPE sites, for videos per year\n",
    "    plt.figure()\n",
    "    sns.lineplot(x=\"year\", y=\"hash\", hue=\"domain\", style=\"domain\", palette = color_palette[type], markers=True, dashes=False, data=filtered_limited_count[type])\n",
    "    plt.title(\"YouTube VIDEOS per year for top %s %s sites\" % (limit,type))\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # count channels by TYPE domain and year\n",
    "    filtered_limited_count = {}\n",
    "    filtered_limited_count[type] = filtered_limited[type].groupby([\"domain\",\"year\"])['channelId'].nunique()\n",
    "    filtered_limited_count[type] = filtered_limited_count[type].to_frame()\n",
    "    filtered_limited_count[type].reset_index(inplace=True)\n",
    "\n",
    "    # plot rise of TYPE sites, for channels per year\n",
    "    plt.figure()\n",
    "    sns.lineplot(x=\"year\", y=\"channelId\", hue=\"domain\", style=\"domain\", palette = color_palette[type], markers=True, dashes=False, data=filtered_limited_count[type])\n",
    "    plt.title(\"YouTube CHANNELS per year for top %s %s sites\" % (limit,type))\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @todo\n",
    "\n",
    "Channel data set\n",
    "* Fuse this with channels data set. Finding a way to use the URLs to characterize channels.\n",
    "   * Channel categories. What hasn't been done at all in the last paper is looking at video categories. That may be done interestingly. Monetization per channel category. Channel classification is not done by video creators. \n",
    "\t   * b = DataFrame(a.var1.str.split('|').tolist(), index=a.var2).stack()\n",
    "\t   * b = b.reset_index()[[0, 'var2']] # var1 variable is currently labeled 0\n",
    "\t   * b.columns = ['var1', 'var2'] # renaming var1\n",
    "   * Merch and monetization per channel. When did they introduce this?\n",
    "   * Whether channels changed merch or crowd-financing, giving that they not backport.\n",
    "* Compare channel categories vs video categories. (But we need to import mysql database.) Is gonna be important when looking at descriptions and tags and such.\n",
    "* Think of categories in terms of scatter plots. E.g. crowd funding vs merch and category. Size is number of videos.\n",
    "* Via topic and/or video categories. E.g. gaming may have a very different URL profile then news/politics. \n",
    "* Merch stores with particular topical affinity (e.g. alt-right channels use store X) - alternative ways of classifying youtube. Otherway around: categories that are more suited to crowd-funding than others.\n",
    "\n",
    "Various\n",
    "* create list of \"commission\"\n",
    "\t* Official YT merch list = https://support.google.com/youtube/answer/6083754?hl=en#zippy=%2Clist-of-approved-merchandise-sites\n",
    "* Domains: open.spotify.com, spotify.com, play.google.com, accounts.google.com, etc. (Does play.google.com has a commission?)\n",
    "\n",
    "Alternative modes of classification? \n",
    "* For a channel we have channel description, keywords, all video titles and keywords. Run as a topic model.\n",
    "* Video categories are unique and chosen by uploaders\n",
    "* Channel category column can have several channels, is assigned done automatically by YouTube\n",
    "* (channels have keywords too, which are much more descriptive. But long-tail, language specific)\n",
    "\n",
    "Later\n",
    "* what kind of channels linked to the academic publishers?\n",
    "* look into what's missing. The \"(note-)book\" of the dead.\n",
    "\t* https://www.youtube.com/error?src=404\n",
    "\t* unacademy.com/unavailable\n",
    "* graphs, instead of absolute numbers, divide into number of videos per year\n",
    "* Patreon pledgers, crowd-funding sites/pledges.\n",
    "* Collect ASINs. How many different products are being advertised (most popular producst - # videos, #views; youtube as amazon catalogue).\n",
    "* Each URL may afford something for analysis.\n",
    "* Cross-language analysis. (Videos have language, channels have country flag). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}